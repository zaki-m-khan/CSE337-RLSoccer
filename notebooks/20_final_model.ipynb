{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RL Soccer Kicker - SARSA & DDPG Experiments\n",
        "\n",
        "## How to Run\n",
        "\n",
        "1. Upload this notebook to [Google Colab](https://colab.research.google.com)\n",
        "2. Run all cells: `Runtime` → `Run all`\n",
        "3. Full run takes ~10-15 minutes\n",
        "\n",
        "## Part 1: SARSA (Open Goal)\n",
        "- SARSA agent with tile coding\n",
        "- 28 discrete actions (7 yaw × 4 speed)\n",
        "- 500 episodes × 3 seeds for reproducibility\n",
        "- Includes random baseline comparison\n",
        "\n",
        "## Part 2: DDPG (vs Goalie)\n",
        "- DDPG agent with PyTorch neural networks\n",
        "- Actor-critic with replay buffer and target networks\n",
        "- Goalie blocking condition\n",
        "\n",
        "---\n",
        "\n",
        "**Settings:** Seeds 0, 42, 123 | ±3° yaw, ±10% speed noise | Goal 2.4m × 1.8m | Reward +5/-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 1: Setup and Dependencies ===\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Reproducibility settings\n",
        "SEEDS = [0, 42, 123]\n",
        "SEED = SEEDS[0]\n",
        "N_EPISODES = 500\n",
        "\n",
        "# Check if running in Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    os.environ['MUJOCO_GL'] = 'egl'\n",
        "    print(\"Installing dependencies...\")\n",
        "    %pip install -q mujoco==3.1.6 gymnasium numpy matplotlib pandas torch --index-url https://download.pytorch.org/whl/cpu\n",
        "    \n",
        "    PROJECT_ROOT = \"/content/soccer_rl\"\n",
        "    os.makedirs(f\"{PROJECT_ROOT}/src\", exist_ok=True)\n",
        "    os.makedirs(f\"{PROJECT_ROOT}/assets\", exist_ok=True)\n",
        "    os.makedirs(f\"{PROJECT_ROOT}/notebooks\", exist_ok=True)\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "    print(f\"Created project at {PROJECT_ROOT}\")\n",
        "else:\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "XML_PATH = os.path.join(PROJECT_ROOT, \"assets\", \"soccer_min.xml\")\n",
        "NOTEBOOKS_PATH = os.path.join(PROJECT_ROOT, \"notebooks\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"SARSA Kicker Training\")\n",
        "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"Master Seed: {SEED}\")\n",
        "print(f\"{'='*50}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 2: Create Source Files (Colab only) ===\n",
        "if IN_COLAB:\n",
        "    # ===== soccer_min.xml =====\n",
        "    soccer_xml = '''<mujoco model=\"soccer_kick_demo\">\n",
        "  <compiler inertiafromgeom=\"true\" angle=\"degree\"/>\n",
        "  <option timestep=\"0.002\" gravity=\"0 0 -9.81\"/>\n",
        "  <worldbody>\n",
        "    <geom name=\"ground\" type=\"plane\" size=\"12 12 0.1\" rgba=\"0.25 0.32 0.25 1\"\n",
        "        friction=\"1.5 0.01 0.01\" condim=\"3\" solimp=\"0.95 0.99 0.001\" solref=\"0.002 1\"/>\n",
        "    <body name=\"goal\" pos=\"5 0 0.9\">\n",
        "      <geom name=\"left_post\"  type=\"box\" size=\"0.05 0.05 0.9\" pos=\"0  1.2 0\" rgba=\"1 1 1 1\"/>\n",
        "      <geom name=\"right_post\" type=\"box\" size=\"0.05 0.05 0.9\" pos=\"0 -1.2 0\" rgba=\"1 1 1 1\"/>\n",
        "      <geom name=\"crossbar\"   type=\"box\" size=\"0.05 1.2 0.05\" pos=\"0 0 0.9\" rgba=\"1 1 1 1\"/>\n",
        "      <geom name=\"net\" type=\"box\" size=\"0.02 1.2 0.9\" pos=\"0 0 0\" rgba=\"0.1 0.1 0.6 0.15\" contype=\"0\" conaffinity=\"0\"/>\n",
        "      <site name=\"target\" pos=\"0 0 0\" size=\"0.08\" rgba=\"1 0 0 0.8\"/>\n",
        "    </body>\n",
        "    <body name=\"ball\" pos=\"-0.35 0 0.11\">\n",
        "        <joint name=\"ball_free\" type=\"free\" damping=\"0.002\"/>\n",
        "        <geom type=\"sphere\" size=\"0.11\" mass=\"0.20\" rgba=\"0.85 0.85 0.85 1\" friction=\"1.0 0.02 0.002\" condim=\"3\"/>\n",
        "    </body>\n",
        "    <body name=\"kicker\" pos=\"-1.0 -0.25 0.11\">\n",
        "        <joint name=\"hinge\" type=\"hinge\" axis=\"0 1 0\" range=\"-35 35\"/>\n",
        "        <geom name=\"foot\" type=\"capsule\" fromto=\"0 0 0 0.3 0 0\" size=\"0.03\" rgba=\"1 0 0 1\"/>\n",
        "        <site name=\"kick_site\" pos=\"0.3 0 0\" size=\"0.02\" rgba=\"1 0 0 1\"/>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor name=\"kick_motor\" joint=\"hinge\" ctrlrange=\"-1 1\" gear=\"300\"/>\n",
        "  </actuator>\n",
        "</mujoco>'''\n",
        "    with open(f\"{PROJECT_ROOT}/assets/soccer_min.xml\", \"w\") as f:\n",
        "        f.write(soccer_xml)\n",
        "    print(\"✓ Created assets/soccer_min.xml\")\n",
        "\n",
        "    # ===== src/soccer_env.py =====\n",
        "    soccer_env_code = '''import mujoco\n",
        "import numpy as np\n",
        "\n",
        "class SoccerKickEnv:\n",
        "    # Execution noise parameters\n",
        "    YAW_NOISE_DEG = 3.0\n",
        "    SPEED_NOISE_PCT = 0.10\n",
        "    \n",
        "    def __init__(self, xml_path, max_steps=3000, dt=0.002, seed=0):\n",
        "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
        "        self.data  = mujoco.MjData(self.model)\n",
        "        self.max_steps = int(max_steps)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        \n",
        "        # Action space: 7 yaw angles × 4 speeds = 28 actions\n",
        "        self.angles_deg = np.array([-12, -8, -4, 0, 4, 8, 12], dtype=np.float32)\n",
        "        self.speeds = np.array([9.0, 11.0, 13.0, 15.0], dtype=np.float32)\n",
        "        self.angles = np.deg2rad(self.angles_deg)\n",
        "        self.actions = [(th, v) for th in self.angles for v in self.speeds]\n",
        "        \n",
        "        self.site_target = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, \"target\")\n",
        "        self.ball_body = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, \"ball\")\n",
        "        self.goal_body = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, \"goal\")\n",
        "        self.goal_x = float(self.model.body_pos[self.goal_body][0])\n",
        "\n",
        "    def _ball_pos(self):\n",
        "        return self.data.qpos[0:3].copy()\n",
        "\n",
        "    def reset(self, target_random=True, seed=None):\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        self.data = mujoco.MjData(self.model)\n",
        "        mujoco.mj_forward(self.model, self.data)\n",
        "        if target_random:\n",
        "            x = self.goal_x + self.rng.uniform(-0.05, 0.05)\n",
        "            y = self.rng.uniform(-0.9, 0.9)\n",
        "            z = self.rng.uniform(0.7, 1.2)\n",
        "            self.model.site_pos[self.site_target] = np.array([x, y, z], dtype=np.float64)\n",
        "            mujoco.mj_forward(self.model, self.data)\n",
        "        return self.model.site_pos[self.site_target].astype(np.float32).copy()\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        idx = int(action_idx) % len(self.actions)\n",
        "        yaw, speed = self.actions[idx]\n",
        "        yaw_deg = np.rad2deg(yaw)\n",
        "        elev = np.deg2rad(20.0)\n",
        "        \n",
        "        # Apply robustness noise: ±3° yaw, ±10% speed\n",
        "        yaw += np.deg2rad(self.rng.uniform(-self.YAW_NOISE_DEG, self.YAW_NOISE_DEG))\n",
        "        speed *= (1.0 + self.rng.uniform(-self.SPEED_NOISE_PCT, self.SPEED_NOISE_PCT))\n",
        "        \n",
        "        vx = float(speed * np.cos(elev) * np.cos(yaw))\n",
        "        vy = float(speed * np.cos(elev) * np.sin(yaw))\n",
        "        vz = float(speed * np.sin(elev))\n",
        "        self.data.qvel[:] = 0\n",
        "        mujoco.mj_forward(self.model, self.data)\n",
        "        self.data.qvel[0:3] = [vx, vy, vz]\n",
        "        crossed, cross_pos = False, None\n",
        "        for _ in range(self.max_steps):\n",
        "            mujoco.mj_step(self.model, self.data)\n",
        "            p = self._ball_pos()\n",
        "            if p[0] >= self.goal_x and not crossed:\n",
        "                crossed, cross_pos = True, p.copy()\n",
        "                break\n",
        "            if np.linalg.norm(self.data.qvel[0:3]) < 0.05 and p[2] < 0.12:\n",
        "                break\n",
        "        # Goal dimensions: 2.4m wide × 1.8m tall\n",
        "        goal_y_half, goal_z_min, goal_z_max = 1.2, 0.0, 1.8\n",
        "        if crossed and cross_pos is not None:\n",
        "            y, z = float(cross_pos[1]), float(cross_pos[2])\n",
        "            success = (-goal_y_half <= y <= goal_y_half) and (goal_z_min <= z <= goal_z_max)\n",
        "            if success:\n",
        "                reward, outcome = 5.0, \"goal\"\n",
        "            else:\n",
        "                reward, outcome = -5.0, \"miss\"\n",
        "        else:\n",
        "            reward, outcome, success = -5.0, \"miss\", False\n",
        "        y_goal = float(cross_pos[1]) if cross_pos is not None else None\n",
        "        z_goal = float(cross_pos[2]) if cross_pos is not None else None\n",
        "        info = {\"success\": success, \"outcome\": outcome, \"y_goal\": y_goal, \"z_goal\": z_goal,\n",
        "                \"yaw_deg\": yaw_deg, \"speed\": float(self.actions[idx][1]), \"action_idx\": idx}\n",
        "        ball = self._ball_pos()\n",
        "        obs = np.array([self.goal_x, 0.0, 1.0], dtype=np.float32) - ball\n",
        "        return obs.astype(np.float32), float(reward), True, info\n",
        "'''\n",
        "    with open(f\"{PROJECT_ROOT}/src/soccer_env.py\", \"w\") as f:\n",
        "        f.write(soccer_env_code)\n",
        "    print(\"✓ Created src/soccer_env.py\")\n",
        "\n",
        "    # ===== src/tilecoding.py =====\n",
        "    tilecoding_code = '''import numpy as np\n",
        "\n",
        "class TileCoder:\n",
        "    def __init__(self, low, high, bins, n_tilings=8, seed=0):\n",
        "        self.low = np.array(low, dtype=np.float32)\n",
        "        self.high = np.array(high, dtype=np.float32)\n",
        "        self.bins = np.array(bins, dtype=np.int32)\n",
        "        self.n_tilings = int(n_tilings)\n",
        "        self.dim = len(self.bins)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.offsets = self.rng.uniform(0.0, 1.0, size=(self.n_tilings, self.dim)).astype(np.float32)\n",
        "        self.features_per_tiling = int(np.prod(self.bins))\n",
        "        self.total_features = self.n_tilings * self.features_per_tiling\n",
        "        self.stride = np.cumprod([1] + list(self.bins[:-1])).astype(np.int32)\n",
        "\n",
        "    def encode(self, s):\n",
        "        s = np.array(s, dtype=np.float32)\n",
        "        u = np.clip((s - self.low) / (self.high - self.low + 1e-8), 0.0, 1.0)\n",
        "        idxs = np.empty(self.n_tilings, dtype=np.int32)\n",
        "        for t in range(self.n_tilings):\n",
        "            shifted = (u + self.offsets[t]) % 1.0\n",
        "            b = np.minimum((shifted * self.bins).astype(np.int32), self.bins - 1)\n",
        "            idxs[t] = t * self.features_per_tiling + int(np.dot(b, self.stride))\n",
        "        return idxs\n",
        "'''\n",
        "    with open(f\"{PROJECT_ROOT}/src/tilecoding.py\", \"w\") as f:\n",
        "        f.write(tilecoding_code)\n",
        "    print(\"✓ Created src/tilecoding.py\")\n",
        "\n",
        "    # ===== src/sarsa_agent.py =====\n",
        "    sarsa_code = '''import numpy as np\n",
        "\n",
        "class SarsaTileAgent:\n",
        "    def __init__(self, n_features, actions, alpha, gamma, eps, seed=0):\n",
        "        self.n_features = n_features\n",
        "        self.actions = np.array(actions)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.W = np.zeros((len(self.actions), n_features), dtype=np.float32)\n",
        "\n",
        "    def select_action(self, f):\n",
        "        if self.rng.random() < self.eps:\n",
        "            return int(self.rng.choice(self.actions))\n",
        "        return int(self.actions[np.argmax(self.W[:, f].sum(axis=1))])\n",
        "\n",
        "    def q_values(self, f):\n",
        "        return self.W[:, f].sum(axis=1)\n",
        "\n",
        "    def update(self, f, a, r, f_next, a_next, done):\n",
        "        q_next = 0.0 if done else self.W[a_next, f_next].sum()\n",
        "        td_error = r + self.gamma * q_next - self.W[a, f].sum()\n",
        "        self.W[a, f] += self.alpha * td_error\n",
        "'''\n",
        "    with open(f\"{PROJECT_ROOT}/src/sarsa_agent.py\", \"w\") as f:\n",
        "        f.write(sarsa_code)\n",
        "    print(\"✓ Created src/sarsa_agent.py\")\n",
        "\n",
        "    with open(f\"{PROJECT_ROOT}/src/__init__.py\", \"w\") as f:\n",
        "        f.write(\"# src module\\n\")\n",
        "    print(\"✓ All source files created!\")\n",
        "else:\n",
        "    print(\"Running locally - using existing source files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 3: Import and Create Environment/Agent ===\n",
        "try:\n",
        "    import mujoco\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', '-q', 'mujoco==3.1.6'])\n",
        "    import mujoco\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from src.soccer_env import SoccerKickEnv\n",
        "from src.tilecoding import TileCoder\n",
        "from src.sarsa_agent import SarsaTileAgent\n",
        "\n",
        "# Create environment\n",
        "env = SoccerKickEnv(XML_PATH, seed=SEED)\n",
        "print(f\"Environment created: {len(env.actions)} discrete actions\")\n",
        "print(f\"Action grid: 7 yaw angles × 4 speeds = 28 actions\")\n",
        "print(f\"  Yaw: {list(env.angles_deg)}°\")\n",
        "print(f\"  Speed: {list(env.speeds)} m/s\")\n",
        "print(f\"Robustness noise: ±{env.YAW_NOISE_DEG}° yaw, ±{int(env.SPEED_NOISE_PCT*100)}% speed\")\n",
        "print(f\"Reward: +5 (goal), -5 (miss) [sparse]\")\n",
        "print(f\"Goal dimensions: width=2.4m (y∈[-1.2,1.2]), height=1.8m (z∈[0,1.8])\")\n",
        "\n",
        "# Print full 28-action list for auditability\n",
        "print(\"\\n=== FULL ACTION LIST (28 actions) ===\")\n",
        "for i, (yaw_rad, speed) in enumerate(env.actions):\n",
        "    yaw_deg = np.rad2deg(yaw_rad)\n",
        "    print(f\"  Action {i:2d}: yaw={yaw_deg:+6.1f}°, speed={speed:5.1f} m/s\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create tile coder (16x16x16 bins, 16 tilings)\n",
        "obs_low = np.array([-6.0, -2.0, -1.0], dtype=np.float32)\n",
        "obs_high = np.array([6.0, 2.0, 2.0], dtype=np.float32)\n",
        "coder = TileCoder(low=obs_low, high=obs_high, bins=[16, 16, 16], n_tilings=16, seed=SEED)\n",
        "print(f\"Tile coder: {coder.total_features} features\")\n",
        "\n",
        "# Create SARSA agent (hyperparameters from report)\n",
        "agent = SarsaTileAgent(\n",
        "    n_features=coder.total_features,\n",
        "    actions=np.arange(len(env.actions)),\n",
        "    alpha=0.02,      # learning rate\n",
        "    gamma=0.95,      # discount factor\n",
        "    eps=0.25,        # initial exploration\n",
        "    seed=SEED\n",
        ")\n",
        "print(f\"SARSA agent: α={agent.alpha}, γ={agent.gamma}, ε₀={agent.eps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 4: SARSA Training Loop (500 episodes × 3 seeds) ===\n",
        "EPS_DECAY = 0.995\n",
        "EPS_MIN = 0.02\n",
        "EPS_START = 0.25\n",
        "\n",
        "# Storage for all seeds\n",
        "all_metrics = {}  # seed -> list of metrics\n",
        "all_shots = {}    # seed -> list of shots\n",
        "\n",
        "print(f\"Training SARSA for {N_EPISODES} episodes × {len(SEEDS)} seeds...\")\n",
        "print(f\"Seeds: {SEEDS}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for seed_idx, seed in enumerate(SEEDS):\n",
        "    print(f\"\\n[Seed {seed}] Starting training...\")\n",
        "    \n",
        "    # Create fresh environment and agent for this seed\n",
        "    env_seed = SoccerKickEnv(XML_PATH, seed=seed)\n",
        "    coder_seed = TileCoder(low=[-6, -3, -2], high=[6, 3, 2], bins=[16, 16, 16], n_tilings=16, seed=seed)\n",
        "    agent_seed = SarsaTileAgent(\n",
        "        n_features=coder_seed.total_features,\n",
        "        actions=list(range(len(env_seed.actions))),\n",
        "        alpha=0.02, gamma=0.95, eps=EPS_START, seed=seed\n",
        "    )\n",
        "    \n",
        "    metrics = []\n",
        "    shots = []\n",
        "    \n",
        "    for ep in range(N_EPISODES):\n",
        "        obs = env_seed.reset(target_random=False, seed=seed + ep)\n",
        "        f = coder_seed.encode(obs)\n",
        "        a = agent_seed.select_action(f)\n",
        "        \n",
        "        # One-shot episode (single kick)\n",
        "        obs_next, reward, done, info = env_seed.step(a)\n",
        "        f_next = coder_seed.encode(obs_next)\n",
        "        a_next = agent_seed.select_action(f_next)\n",
        "        \n",
        "        # SARSA update\n",
        "        agent_seed.update(f, a, reward, f_next, a_next, done)\n",
        "        \n",
        "        # Epsilon decay\n",
        "        agent_seed.eps = max(EPS_MIN, agent_seed.eps * EPS_DECAY)\n",
        "        \n",
        "        # Log metrics\n",
        "        metrics.append({\n",
        "            'seed': seed,\n",
        "            'episode': ep,\n",
        "            'reward': reward,\n",
        "            'success': int(info['success']),\n",
        "            'epsilon': agent_seed.eps,\n",
        "            'action_idx': info.get('action_idx', a)\n",
        "        })\n",
        "        \n",
        "        # Log shot data\n",
        "        shots.append({\n",
        "            'seed': seed,\n",
        "            'episode': ep,\n",
        "            'y_goal': info['y_goal'],\n",
        "            'z_goal': info['z_goal'],\n",
        "            'outcome': info['outcome'],\n",
        "            'yaw_deg': info['yaw_deg'],\n",
        "            'speed': info['speed']\n",
        "        })\n",
        "        \n",
        "        # Progress output\n",
        "        if ep % 100 == 0 or ep == N_EPISODES - 1:\n",
        "            recent = metrics[-50:] if len(metrics) >= 50 else metrics\n",
        "            avg_reward = np.mean([m['reward'] for m in recent])\n",
        "            avg_success = np.mean([m['success'] for m in recent]) * 100\n",
        "            print(f\"  Ep {ep:3d} | Reward: {avg_reward:6.2f} | Success: {avg_success:5.1f}% | ε: {agent_seed.eps:.3f}\")\n",
        "    \n",
        "    total_successes = sum(m['success'] for m in metrics)\n",
        "    print(f\"  [Seed {seed}] Done: {total_successes}/{N_EPISODES} goals ({100*total_successes/N_EPISODES:.1f}%)\")\n",
        "    \n",
        "    all_metrics[seed] = metrics\n",
        "    all_shots[seed] = shots\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ All seeds complete!\")\n",
        "for seed in SEEDS:\n",
        "    successes = sum(m['success'] for m in all_metrics[seed])\n",
        "    print(f\"  Seed {seed:3d}: {successes}/{N_EPISODES} goals ({100*successes/N_EPISODES:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 5: Save Multi-Seed Metrics to CSV ===\n",
        "# Combine all seeds into single DataFrames\n",
        "all_metrics_list = []\n",
        "all_shots_list = []\n",
        "for seed in SEEDS:\n",
        "    all_metrics_list.extend(all_metrics[seed])\n",
        "    all_shots_list.extend(all_shots[seed])\n",
        "\n",
        "df_metrics = pd.DataFrame(all_metrics_list)\n",
        "df_shots = pd.DataFrame(all_shots_list)\n",
        "\n",
        "# Save to CSV (includes seed column)\n",
        "metrics_path = os.path.join(NOTEBOOKS_PATH, \"sarsa_open_goal_metrics.csv\")\n",
        "shots_path = os.path.join(NOTEBOOKS_PATH, \"sarsa_shots_open_goal.csv\")\n",
        "\n",
        "df_metrics.to_csv(metrics_path, index=False)\n",
        "df_shots.to_csv(shots_path, index=False)\n",
        "\n",
        "print(f\"✓ Saved metrics to: {metrics_path}\")\n",
        "print(f\"✓ Saved shots to: {shots_path}\")\n",
        "print(f\"\\nMetrics shape: {df_metrics.shape}\")\n",
        "print(f\"Shots shape: {df_shots.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 6: Plot Multi-Seed Learning Curves (Mean ± Std) ===\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "WINDOW = 50\n",
        "\n",
        "# Compute rolling metrics for each seed\n",
        "rolling_rewards = {}\n",
        "rolling_successes = {}\n",
        "for seed in SEEDS:\n",
        "    seed_data = df_metrics[df_metrics['seed'] == seed].sort_values('episode')\n",
        "    rolling_rewards[seed] = pd.Series(seed_data['reward'].values).rolling(WINDOW, min_periods=1).mean().values\n",
        "    rolling_successes[seed] = pd.Series(seed_data['success'].values).rolling(WINDOW, min_periods=1).mean().values * 100\n",
        "\n",
        "# Stack for computing mean/std\n",
        "reward_matrix = np.array([rolling_rewards[s] for s in SEEDS])\n",
        "success_matrix = np.array([rolling_successes[s] for s in SEEDS])\n",
        "episodes = np.arange(N_EPISODES)\n",
        "\n",
        "mean_reward = reward_matrix.mean(axis=0)\n",
        "std_reward = reward_matrix.std(axis=0)\n",
        "mean_success = success_matrix.mean(axis=0)\n",
        "std_success = success_matrix.std(axis=0)\n",
        "\n",
        "# Plot 1: Reward with confidence band\n",
        "axes[0].plot(episodes, mean_reward, color='#2E86AB', linewidth=2, label='Mean')\n",
        "axes[0].fill_between(episodes, mean_reward - std_reward, mean_reward + std_reward, \n",
        "                     color='#2E86AB', alpha=0.2, label='±1 Std')\n",
        "for seed in SEEDS:\n",
        "    axes[0].plot(episodes, rolling_rewards[seed], alpha=0.3, linewidth=1, label=f'Seed {seed}')\n",
        "axes[0].set_xlabel('Episode', fontsize=12)\n",
        "axes[0].set_ylabel('Reward (rolling avg)', fontsize=12)\n",
        "axes[0].set_title(f'SARSA Learning Curve (3 seeds, window={WINDOW})', fontsize=14)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(y=5.0, color='green', linestyle='--', alpha=0.5, label='Max reward')\n",
        "axes[0].legend(loc='lower right', fontsize=9)\n",
        "\n",
        "# Plot 2: Success Rate with confidence band\n",
        "axes[1].plot(episodes, mean_success, color='#28A745', linewidth=2, label='Mean')\n",
        "axes[1].fill_between(episodes, mean_success - std_success, mean_success + std_success,\n",
        "                     color='#28A745', alpha=0.2, label='±1 Std')\n",
        "for seed in SEEDS:\n",
        "    axes[1].plot(episodes, rolling_successes[seed], alpha=0.3, linewidth=1, label=f'Seed {seed}')\n",
        "axes[1].set_xlabel('Episode', fontsize=12)\n",
        "axes[1].set_ylabel('Success Rate %', fontsize=12)\n",
        "axes[1].set_title(f'SARSA Success Rate (3 seeds, window={WINDOW})', fontsize=14)\n",
        "axes[1].set_ylim(0, 105)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='90% threshold')\n",
        "axes[1].legend(loc='lower right', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(NOTEBOOKS_PATH, \"fig_sarsa_learning.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: fig_sarsa_learning.png\")\n",
        "print(f\"Low variance across seeds: Std at ep500 = {std_success[-1]:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 7: Shot Scatter Plot (Goal Frame View) - All Seeds Combined ===\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Filter shots that crossed the goal plane (all seeds combined)\n",
        "df_valid = df_shots.dropna(subset=['y_goal', 'z_goal'])\n",
        "total_shots = len(df_valid)\n",
        "\n",
        "# Separate goals and misses\n",
        "goals = df_valid[df_valid['outcome'] == 'goal']\n",
        "misses = df_valid[df_valid['outcome'] == 'miss']\n",
        "\n",
        "# Plot shots\n",
        "ax.scatter(misses['y_goal'], misses['z_goal'], c='red', alpha=0.4, s=30, label=f'Miss ({len(misses)})')\n",
        "ax.scatter(goals['y_goal'], goals['z_goal'], c='green', alpha=0.6, s=50, label=f'Goal ({len(goals)})')\n",
        "\n",
        "# Draw goal frame\n",
        "goal_y_half = 1.2\n",
        "goal_z_bottom = 0.0\n",
        "goal_z_top = 1.8\n",
        "\n",
        "# Posts\n",
        "ax.plot([-goal_y_half, -goal_y_half], [goal_z_bottom, goal_z_top], 'k-', linewidth=4)\n",
        "ax.plot([goal_y_half, goal_y_half], [goal_z_bottom, goal_z_top], 'k-', linewidth=4)\n",
        "# Crossbar\n",
        "ax.plot([-goal_y_half, goal_y_half], [goal_z_top, goal_z_top], 'k-', linewidth=4)\n",
        "# Ground line\n",
        "ax.plot([-goal_y_half, goal_y_half], [goal_z_bottom, goal_z_bottom], 'k-', linewidth=2, alpha=0.5)\n",
        "\n",
        "# Goal area fill\n",
        "from matplotlib.patches import Rectangle\n",
        "goal_rect = Rectangle((-goal_y_half, goal_z_bottom), 2*goal_y_half, goal_z_top - goal_z_bottom, \n",
        "                       fill=True, facecolor='lightgreen', alpha=0.2, edgecolor='none')\n",
        "ax.add_patch(goal_rect)\n",
        "\n",
        "ax.set_xlim(-2.0, 2.0)\n",
        "ax.set_ylim(0, 2.2)\n",
        "ax.set_xlabel('Y position (m)', fontsize=12)\n",
        "ax.set_ylabel('Z position (m)', fontsize=12)\n",
        "ax.set_title('SARSA Shot Distribution (Open Goal)', fontsize=14)\n",
        "ax.set_aspect('equal')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.savefig(os.path.join(NOTEBOOKS_PATH, \"fig_sarsa_shots.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: fig_sarsa_shots.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 8: Random Baseline (3 seeds) ===\n",
        "print(f\"Running Random Policy baseline ({N_EPISODES} episodes × {len(SEEDS)} seeds)...\")\n",
        "print(f\"Uniform sampling from {len(env.actions)} discrete actions\")\n",
        "\n",
        "all_random_metrics = {}\n",
        "for seed in SEEDS:\n",
        "    # Create fresh environment with offset seed for random baseline\n",
        "    env_random = SoccerKickEnv(XML_PATH, seed=seed + 1000)\n",
        "    rng_random = np.random.default_rng(seed + 1000)\n",
        "    \n",
        "    random_metrics = []\n",
        "    for ep in range(N_EPISODES):\n",
        "        env_random.reset(target_random=False, seed=seed + 1000 + ep)\n",
        "        action = rng_random.integers(0, len(env_random.actions))  # Uniform from 28 actions\n",
        "        _, reward, _, info = env_random.step(action)\n",
        "        random_metrics.append({\n",
        "            'seed': seed,\n",
        "            'episode': ep,\n",
        "            'reward': reward,\n",
        "            'success': int(info['success'])\n",
        "        })\n",
        "    all_random_metrics[seed] = random_metrics\n",
        "    successes = sum(m['success'] for m in random_metrics)\n",
        "    print(f\"  Seed {seed}: {successes}/{N_EPISODES} goals ({100*successes/N_EPISODES:.1f}%)\")\n",
        "\n",
        "# Combine and save\n",
        "random_list = []\n",
        "for seed in SEEDS:\n",
        "    random_list.extend(all_random_metrics[seed])\n",
        "df_random = pd.DataFrame(random_list)\n",
        "random_path = os.path.join(NOTEBOOKS_PATH, \"random_baseline_metrics.csv\")\n",
        "df_random.to_csv(random_path, index=False)\n",
        "\n",
        "random_successes_total = df_random['success'].sum()\n",
        "total_random_eps = len(SEEDS) * N_EPISODES\n",
        "print(f\"\\n✓ Random baseline total: {random_successes_total}/{total_random_eps} ({100*random_successes_total/total_random_eps:.1f}%)\")\n",
        "print(f\"✓ Saved: {random_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 9: SARSA vs Random Comparison Plot (Multi-Seed Mean ± Std) ===\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Compute rolling success for each seed - SARSA\n",
        "rolling_sarsa = {}\n",
        "rolling_random = {}\n",
        "for seed in SEEDS:\n",
        "    sarsa_data = df_metrics[df_metrics['seed'] == seed].sort_values('episode')\n",
        "    random_data = df_random[df_random['seed'] == seed].sort_values('episode')\n",
        "    rolling_sarsa[seed] = pd.Series(sarsa_data['success'].values).rolling(WINDOW, min_periods=1).mean().values * 100\n",
        "    rolling_random[seed] = pd.Series(random_data['success'].values).rolling(WINDOW, min_periods=1).mean().values * 100\n",
        "\n",
        "# Compute mean/std\n",
        "sarsa_matrix = np.array([rolling_sarsa[s] for s in SEEDS])\n",
        "random_matrix = np.array([rolling_random[s] for s in SEEDS])\n",
        "mean_sarsa = sarsa_matrix.mean(axis=0)\n",
        "std_sarsa = sarsa_matrix.std(axis=0)\n",
        "mean_random = random_matrix.mean(axis=0)\n",
        "std_random = random_matrix.std(axis=0)\n",
        "\n",
        "# Compute summary stats\n",
        "total_sarsa_successes = df_metrics['success'].sum()\n",
        "total_sarsa_eps = len(SEEDS) * N_EPISODES\n",
        "sarsa_rate = 100 * total_sarsa_successes / total_sarsa_eps\n",
        "random_rate = 100 * random_successes_total / total_random_eps\n",
        "\n",
        "# Plot with bands\n",
        "ax.plot(episodes, mean_sarsa, color='#2E86AB', linewidth=2.5, label=f'SARSA (mean: {sarsa_rate:.1f}%)')\n",
        "ax.fill_between(episodes, mean_sarsa - std_sarsa, mean_sarsa + std_sarsa, color='#2E86AB', alpha=0.2)\n",
        "ax.plot(episodes, mean_random, color='#DC3545', linewidth=2.5, label=f'Random (mean: {random_rate:.1f}%)')\n",
        "ax.fill_between(episodes, mean_random - std_random, mean_random + std_random, color='#DC3545', alpha=0.2)\n",
        "\n",
        "ax.set_xlabel('Episode', fontsize=12)\n",
        "ax.set_ylabel('Success Rate % (rolling avg)', fontsize=12)\n",
        "ax.set_title('SARSA vs Random Policy (3 seeds, mean ± std)', fontsize=14)\n",
        "ax.set_ylim(0, 105)\n",
        "ax.legend(loc='center right', fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.savefig(os.path.join(NOTEBOOKS_PATH, \"fig_sarsa_vs_random.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: fig_sarsa_vs_random.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 10: SARSA Summary Table ===\n",
        "print(\"=\" * 70)\n",
        "print(\"SARSA TRAINING SUMMARY (Open Goal)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Per-seed results\n",
        "print(\"\\nPer-Seed Results:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Seed':<10} {'Goals':>10} {'Total':>10} {'Rate':>10}\")\n",
        "print(\"-\" * 50)\n",
        "for seed in SEEDS:\n",
        "    seed_successes = sum(m['success'] for m in all_metrics[seed])\n",
        "    print(f\"{seed:<10} {seed_successes:>10} {N_EPISODES:>10} {100*seed_successes/N_EPISODES:>9.1f}%\")\n",
        "print(\"-\" * 50)\n",
        "avg_sarsa = np.mean([sum(m['success'] for m in all_metrics[s])/N_EPISODES*100 for s in SEEDS])\n",
        "std_sarsa = np.std([sum(m['success'] for m in all_metrics[s])/N_EPISODES*100 for s in SEEDS])\n",
        "print(f\"{'Average':<10} {'-':>10} {'-':>10} {avg_sarsa:>7.1f}% (±{std_sarsa:.1f})\")\n",
        "\n",
        "# Random baseline comparison\n",
        "avg_random = np.mean([sum(m['success'] for m in all_random_metrics[s])/N_EPISODES*100 for s in SEEDS])\n",
        "print(f\"{'Random':<10} {'-':>10} {'-':>10} {avg_random:>9.1f}%\")\n",
        "print(f\"{'Improvement':<10} {'-':>10} {'-':>10} +{avg_sarsa - avg_random:>6.1f}%\")\n",
        "\n",
        "# Best action analysis\n",
        "action_counts = df_metrics[df_metrics['success'] == 1]['action_idx'].value_counts()\n",
        "if len(action_counts) > 0:\n",
        "    best_action_idx = action_counts.index[0]\n",
        "    best_yaw, best_speed = env.actions[best_action_idx]\n",
        "    print(f\"\\nMost successful action: idx={best_action_idx}\")\n",
        "    print(f\"  → yaw={np.rad2deg(best_yaw):.0f}°, speed={best_speed:.0f} m/s\")\n",
        "\n",
        "print(f\"\\nFiles saved to notebooks/\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: DDPG vs Goalie\n",
        "\n",
        "DDPG agent with actor-critic neural networks, replay buffer, and target networks.\n",
        "The goalie randomly positions itself in part of the goal each episode.\n",
        "\n",
        "**Architecture:**\n",
        "- Actor/Critic: 2-layer MLP (64 hidden units)\n",
        "- Target networks with soft updates (τ=0.005)\n",
        "- Replay buffer: 10K capacity, batch size 64\n",
        "- Same 28-action space as SARSA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 12: Create Goalie Environment ===\n",
        "\n",
        "class SoccerKickEnvGoalie:\n",
        "    \"\"\"Soccer environment with a goalie blocking zone.\"\"\"\n",
        "    \n",
        "    YAW_NOISE_DEG = 3.0\n",
        "    SPEED_NOISE_PCT = 0.10\n",
        "    \n",
        "    def __init__(self, xml_path, max_steps=3000, seed=0, goalie_width=0.6):\n",
        "        import mujoco\n",
        "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
        "        self.data = mujoco.MjData(self.model)\n",
        "        self.max_steps = int(max_steps)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.goalie_width = goalie_width\n",
        "        \n",
        "        # Action space: 7 yaw × 4 speed = 28 actions\n",
        "        self.angles_deg = np.array([-12, -8, -4, 0, 4, 8, 12], dtype=np.float32)\n",
        "        self.speeds = np.array([9.0, 11.0, 13.0, 15.0], dtype=np.float32)\n",
        "        self.angles = np.deg2rad(self.angles_deg)\n",
        "        self.actions = [(th, v) for th in self.angles for v in self.speeds]\n",
        "        \n",
        "        self.goal_body = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, \"goal\")\n",
        "        self.goal_x = float(self.model.body_pos[self.goal_body][0])\n",
        "        \n",
        "        # Goalie position (randomized each episode)\n",
        "        self.goalie_y = 0.0\n",
        "        self.goalie_z = 0.9\n",
        "        \n",
        "    def _ball_pos(self):\n",
        "        return self.data.qpos[0:3].copy()\n",
        "    \n",
        "    def reset(self, seed=None):\n",
        "        import mujoco\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        self.data = mujoco.MjData(self.model)\n",
        "        mujoco.mj_forward(self.model, self.data)\n",
        "        \n",
        "        # Randomize goalie position within goal\n",
        "        self.goalie_y = self.rng.uniform(-0.7, 0.7)\n",
        "        self.goalie_z = self.rng.uniform(0.5, 1.3)\n",
        "        \n",
        "        ball = self._ball_pos()\n",
        "        obs = np.array([self.goal_x - ball[0], -ball[1], 1.0 - ball[2], \n",
        "                       self.goalie_y, self.goalie_z], dtype=np.float32)\n",
        "        return obs\n",
        "    \n",
        "    def step(self, action_idx):\n",
        "        import mujoco\n",
        "        idx = int(action_idx) % len(self.actions)\n",
        "        yaw, speed = self.actions[idx]\n",
        "        yaw_deg = np.rad2deg(yaw)\n",
        "        \n",
        "        elev = np.deg2rad(20.0)\n",
        "        # Apply robustness noise: ±3° yaw, ±10% speed\n",
        "        yaw += np.deg2rad(self.rng.uniform(-self.YAW_NOISE_DEG, self.YAW_NOISE_DEG))\n",
        "        speed *= (1.0 + self.rng.uniform(-self.SPEED_NOISE_PCT, self.SPEED_NOISE_PCT))\n",
        "        \n",
        "        vx = float(speed * np.cos(elev) * np.cos(yaw))\n",
        "        vy = float(speed * np.cos(elev) * np.sin(yaw))\n",
        "        vz = float(speed * np.sin(elev))\n",
        "        \n",
        "        self.data.qvel[:] = 0\n",
        "        mujoco.mj_forward(self.model, self.data)\n",
        "        self.data.qvel[0:3] = [vx, vy, vz]\n",
        "        \n",
        "        crossed, cross_pos = False, None\n",
        "        for _ in range(self.max_steps):\n",
        "            mujoco.mj_step(self.model, self.data)\n",
        "            p = self._ball_pos()\n",
        "            if p[0] >= self.goal_x and not crossed:\n",
        "                crossed, cross_pos = True, p.copy()\n",
        "                break\n",
        "            if np.linalg.norm(self.data.qvel[0:3]) < 0.05 and p[2] < 0.12:\n",
        "                break\n",
        "        \n",
        "        # Goal dimensions: 2.4m wide × 1.8m tall\n",
        "        goal_y_half, goal_z_min, goal_z_max = 1.2, 0.0, 1.8\n",
        "        \n",
        "        if crossed and cross_pos is not None:\n",
        "            y, z = float(cross_pos[1]), float(cross_pos[2])\n",
        "            in_goal = (-goal_y_half <= y <= goal_y_half) and (goal_z_min <= z <= goal_z_max)\n",
        "            \n",
        "            blocked = (abs(y - self.goalie_y) < self.goalie_width/2 and \n",
        "                      abs(z - self.goalie_z) < 0.4)\n",
        "            \n",
        "            if in_goal and not blocked:\n",
        "                reward, outcome, success = 5.0, \"goal\", True   # +5 for goal\n",
        "            elif blocked:\n",
        "                reward, outcome, success = -5.0, \"blocked\", False  # -5 for blocked\n",
        "            else:\n",
        "                reward, outcome, success = -5.0, \"miss\", False  # -5 for miss\n",
        "        else:\n",
        "            reward, outcome, success = -5.0, \"miss\", False  # -5 for miss\n",
        "        \n",
        "        y_goal = float(cross_pos[1]) if cross_pos is not None else None\n",
        "        z_goal = float(cross_pos[2]) if cross_pos is not None else None\n",
        "        \n",
        "        info = {\"success\": success, \"outcome\": outcome, \"y_goal\": y_goal, \"z_goal\": z_goal,\n",
        "                \"yaw_deg\": yaw_deg, \"speed\": float(self.actions[idx][1]), \"action_idx\": idx,\n",
        "                \"goalie_y\": self.goalie_y, \"goalie_z\": self.goalie_z}\n",
        "        \n",
        "        ball = self._ball_pos()\n",
        "        obs = np.array([self.goal_x - ball[0], -ball[1], 1.0 - ball[2],\n",
        "                       self.goalie_y, self.goalie_z], dtype=np.float32)\n",
        "        return obs, float(reward), True, info\n",
        "\n",
        "# Create goalie environment\n",
        "env_goalie = SoccerKickEnvGoalie(XML_PATH, seed=SEED + 2000)\n",
        "print(f\"✓ Goalie environment created\")\n",
        "print(f\"  Goalie width: {env_goalie.goalie_width}m (blocks part of goal)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 13: PyTorch DDPG Agent ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Deterministic settings\n",
        "torch.set_default_device('cpu')\n",
        "torch.manual_seed(SEED + 2000)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, seed=0):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.rng = random.Random(seed)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch = self.rng.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (torch.FloatTensor(np.array(states)),\n",
        "                torch.LongTensor(actions),\n",
        "                torch.FloatTensor(rewards),\n",
        "                torch.FloatTensor(np.array(next_states)),\n",
        "                torch.FloatTensor(dones))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, n_actions)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, n_actions)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DDPGAgent:\n",
        "    def __init__(self, obs_dim, n_actions, lr_actor=1e-3, lr_critic=1e-3,\n",
        "                 gamma=0.95, tau=0.005, buffer_size=10000, batch_size=64,\n",
        "                 noise_scale=0.5, seed=0):\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.noise_scale = noise_scale\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        \n",
        "        # Actor networks\n",
        "        self.actor = ActorNetwork(obs_dim, n_actions)\n",
        "        self.actor_target = ActorNetwork(obs_dim, n_actions)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "        \n",
        "        # Critic networks\n",
        "        self.critic = CriticNetwork(obs_dim, n_actions)\n",
        "        self.critic_target = CriticNetwork(obs_dim, n_actions)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
        "        \n",
        "        # Replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size, seed=seed)\n",
        "        \n",
        "    def select_action(self, obs, explore=True):\n",
        "        with torch.no_grad():\n",
        "            obs_t = torch.FloatTensor(obs).unsqueeze(0)\n",
        "            logits = self.actor(obs_t).squeeze(0).numpy()\n",
        "        \n",
        "        if explore:\n",
        "            # Add exploration noise to logits\n",
        "            noise = self.rng.normal(0, self.noise_scale, size=self.n_actions)\n",
        "            logits = logits + noise\n",
        "        \n",
        "        # Softmax to get probabilities\n",
        "        probs = np.exp(logits - logits.max())\n",
        "        probs = probs / probs.sum()\n",
        "        \n",
        "        if explore:\n",
        "            return int(self.rng.choice(self.n_actions, p=probs))\n",
        "        else:\n",
        "            return int(np.argmax(probs))\n",
        "    \n",
        "    def store_transition(self, obs, action, reward, next_obs, done):\n",
        "        self.replay_buffer.push(obs, action, reward, next_obs, float(done))\n",
        "    \n",
        "    def update(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        \n",
        "        # Critic update\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.critic_target(next_states)\n",
        "            next_q_max = next_q_values.max(dim=1)[0]\n",
        "            target_q = rewards + self.gamma * next_q_max * (1 - dones)\n",
        "        \n",
        "        current_q = self.critic(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
        "        \n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        \n",
        "        # Actor update (maximize Q-value of selected actions)\n",
        "        actor_logits = self.actor(states)\n",
        "        actor_probs = torch.softmax(actor_logits, dim=1)\n",
        "        q_values = self.critic(states)\n",
        "        actor_loss = -(actor_probs * q_values).sum(dim=1).mean()\n",
        "        \n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Soft update target networks\n",
        "        self._soft_update(self.actor, self.actor_target)\n",
        "        self._soft_update(self.critic, self.critic_target)\n",
        "    \n",
        "    def _soft_update(self, source, target):\n",
        "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "# Create DDPG agent (5D observation: ball_rel + goalie_pos)\n",
        "ddpg_agent = DDPGAgent(\n",
        "    obs_dim=5,\n",
        "    n_actions=len(env_goalie.actions),  # 28 actions\n",
        "    lr_actor=1e-3,\n",
        "    lr_critic=1e-3,\n",
        "    gamma=0.95,\n",
        "    tau=0.005,\n",
        "    buffer_size=10000,\n",
        "    batch_size=64,\n",
        "    noise_scale=0.5,\n",
        "    seed=SEED + 2000\n",
        ")\n",
        "print(f\"✓ DDPG agent created (PyTorch)\")\n",
        "print(f\"  Observation dim: 5 (ball_rel[3] + goalie_pos[2])\")\n",
        "print(f\"  Actions: {ddpg_agent.n_actions}\")\n",
        "print(f\"  Actor: 5→64→64→28 (ReLU)\")\n",
        "print(f\"  Critic: 5→64→64→28 (ReLU)\")\n",
        "print(f\"  Replay buffer: {ddpg_agent.replay_buffer.buffer.maxlen} capacity\")\n",
        "print(f\"  Batch size: {ddpg_agent.batch_size}\")\n",
        "print(f\"  Target update τ: {ddpg_agent.tau}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 14: DDPG Training Loop (vs Goalie) ===\n",
        "DDPG_EPISODES = 500\n",
        "\n",
        "ddpg_metrics = []\n",
        "ddpg_shots = []\n",
        "\n",
        "print(f\"Training DDPG for {DDPG_EPISODES} episodes (vs Goalie)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for ep in range(DDPG_EPISODES):\n",
        "    obs = env_goalie.reset(seed=SEED + 2000 + ep)\n",
        "    action = ddpg_agent.select_action(obs, explore=True)\n",
        "    \n",
        "    next_obs, reward, done, info = env_goalie.step(action)\n",
        "    \n",
        "    ddpg_agent.store_transition(obs, action, reward, next_obs, done)\n",
        "    ddpg_agent.update()\n",
        "    ddpg_agent.noise_scale = max(0.1, ddpg_agent.noise_scale * 0.997)\n",
        "    \n",
        "    ddpg_metrics.append({\n",
        "        'episode': ep,\n",
        "        'reward': reward,\n",
        "        'success': int(info['success']),\n",
        "        'outcome': info['outcome'],\n",
        "        'action_idx': info['action_idx']\n",
        "    })\n",
        "    ddpg_shots.append({\n",
        "        'episode': ep,\n",
        "        'y_goal': info['y_goal'],\n",
        "        'z_goal': info['z_goal'],\n",
        "        'outcome': info['outcome'],\n",
        "        'goalie_y': info['goalie_y'],\n",
        "        'goalie_z': info['goalie_z']\n",
        "    })\n",
        "    \n",
        "    if ep % 100 == 0 or ep == DDPG_EPISODES - 1:\n",
        "        recent = ddpg_metrics[-50:] if len(ddpg_metrics) >= 50 else ddpg_metrics\n",
        "        avg_reward = np.mean([m['reward'] for m in recent])\n",
        "        avg_success = np.mean([m['success'] for m in recent]) * 100\n",
        "        blocked = sum(1 for m in recent if m['outcome'] == 'blocked')\n",
        "        print(f\"Episode {ep:3d} | Reward: {avg_reward:6.2f} | Success: {avg_success:5.1f}% | Blocked: {blocked} | Noise: {ddpg_agent.noise_scale:.2f}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "ddpg_successes = sum(m['success'] for m in ddpg_metrics)\n",
        "ddpg_blocked = sum(1 for m in ddpg_metrics if m['outcome'] == 'blocked')\n",
        "print(f\"✓ DDPG complete: {ddpg_successes}/{DDPG_EPISODES} goals ({100*ddpg_successes/DDPG_EPISODES:.1f}%)\")\n",
        "print(f\"  Blocked by goalie: {ddpg_blocked} shots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 15: Save DDPG Metrics ===\n",
        "df_ddpg_metrics = pd.DataFrame(ddpg_metrics)\n",
        "df_ddpg_shots = pd.DataFrame(ddpg_shots)\n",
        "\n",
        "ddpg_metrics_path = os.path.join(NOTEBOOKS_PATH, \"ddpg_goalie_metrics.csv\")\n",
        "ddpg_shots_path = os.path.join(NOTEBOOKS_PATH, \"ddpg_shots_goalie.csv\")\n",
        "\n",
        "df_ddpg_metrics.to_csv(ddpg_metrics_path, index=False)\n",
        "df_ddpg_shots.to_csv(ddpg_shots_path, index=False)\n",
        "\n",
        "print(f\"✓ Saved DDPG metrics to: {ddpg_metrics_path}\")\n",
        "print(f\"✓ Saved DDPG shots to: {ddpg_shots_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 16: DDPG Learning Curves ===\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Rolling Average Reward\n",
        "ddpg_rewards = df_ddpg_metrics['reward'].values\n",
        "ddpg_rolling_reward = pd.Series(ddpg_rewards).rolling(WINDOW, min_periods=1).mean()\n",
        "axes[0].plot(ddpg_rolling_reward, color='#9B59B6', linewidth=2)\n",
        "axes[0].set_xlabel('Episode', fontsize=12)\n",
        "axes[0].set_ylabel('Reward (rolling avg)', fontsize=12)\n",
        "axes[0].set_title(f'DDPG Learning Curve vs Goalie (window={WINDOW})', fontsize=14)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(y=5.0, color='green', linestyle='--', alpha=0.5, label='Max reward (+5)')\n",
        "axes[0].axhline(y=-5.0, color='red', linestyle='--', alpha=0.5, label='Miss penalty (-5)')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: Rolling Success Rate\n",
        "ddpg_success = df_ddpg_metrics['success'].values\n",
        "ddpg_rolling_success = pd.Series(ddpg_success).rolling(WINDOW, min_periods=1).mean() * 100\n",
        "axes[1].plot(ddpg_rolling_success, color='#E74C3C', linewidth=2)\n",
        "axes[1].set_xlabel('Episode', fontsize=12)\n",
        "axes[1].set_ylabel('Success Rate %', fontsize=12)\n",
        "axes[1].set_title(f'DDPG Success Rate vs Goalie (window={WINDOW})', fontsize=14)\n",
        "axes[1].set_ylim(0, 105)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(NOTEBOOKS_PATH, \"fig_ddpg_learning.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: fig_ddpg_learning.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 17: DDPG Shot Scatter Plot (vs Goalie) ===\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "df_ddpg_valid = df_ddpg_shots.dropna(subset=['y_goal', 'z_goal'])\n",
        "\n",
        "# Separate by outcome\n",
        "goals_ddpg = df_ddpg_valid[df_ddpg_valid['outcome'] == 'goal']\n",
        "misses_ddpg = df_ddpg_valid[df_ddpg_valid['outcome'] == 'miss']\n",
        "blocked_ddpg = df_ddpg_valid[df_ddpg_valid['outcome'] == 'blocked']\n",
        "\n",
        "# Plot shots\n",
        "ax.scatter(misses_ddpg['y_goal'], misses_ddpg['z_goal'], c='red', alpha=0.3, s=25, label=f'Miss ({len(misses_ddpg)})')\n",
        "ax.scatter(blocked_ddpg['y_goal'], blocked_ddpg['z_goal'], c='orange', alpha=0.5, s=40, marker='x', label=f'Blocked ({len(blocked_ddpg)})')\n",
        "ax.scatter(goals_ddpg['y_goal'], goals_ddpg['z_goal'], c='green', alpha=0.6, s=50, label=f'Goal ({len(goals_ddpg)})')\n",
        "\n",
        "# Draw goal frame\n",
        "goal_y_half = 1.2\n",
        "goal_z_bottom, goal_z_top = 0.0, 1.8\n",
        "ax.plot([-goal_y_half, -goal_y_half], [goal_z_bottom, goal_z_top], 'k-', linewidth=4)\n",
        "ax.plot([goal_y_half, goal_y_half], [goal_z_bottom, goal_z_top], 'k-', linewidth=4)\n",
        "ax.plot([-goal_y_half, goal_y_half], [goal_z_top, goal_z_top], 'k-', linewidth=4)\n",
        "ax.plot([-goal_y_half, goal_y_half], [goal_z_bottom, goal_z_bottom], 'k-', linewidth=2, alpha=0.5)\n",
        "\n",
        "goal_rect = Rectangle((-goal_y_half, goal_z_bottom), 2*goal_y_half, goal_z_top - goal_z_bottom, \n",
        "                       fill=True, facecolor='lightgreen', alpha=0.15, edgecolor='none')\n",
        "ax.add_patch(goal_rect)\n",
        "\n",
        "ax.set_xlim(-2.0, 2.0)\n",
        "ax.set_ylim(0, 2.2)\n",
        "ax.set_xlabel('Y position (m)', fontsize=12)\n",
        "ax.set_ylabel('Z position (m)', fontsize=12)\n",
        "ax.set_title('DDPG Shot Distribution (vs Goalie)', fontsize=14)\n",
        "ax.set_aspect('equal')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.savefig(os.path.join(NOTEBOOKS_PATH, \"fig_ddpg_shots.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: fig_ddpg_shots.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 18: SARSA vs DDPG Comparison ===\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Use mean SARSA success (averaged across seeds)\n",
        "sarsa_rolling = mean_sarsa  # Already computed in Cell 6\n",
        "# DDPG (vs goalie)\n",
        "ddpg_rolling = pd.Series(df_ddpg_metrics['success'].values).rolling(WINDOW, min_periods=1).mean() * 100\n",
        "\n",
        "ax.plot(episodes, sarsa_rolling, color='#2E86AB', linewidth=2.5, label=f'SARSA Open Goal (mean: {avg_sarsa:.1f}%)')\n",
        "ax.plot(ddpg_rolling.values, color='#9B59B6', linewidth=2.5, label=f'DDPG vs Goalie: {100*ddpg_successes/DDPG_EPISODES:.1f}%')\n",
        "\n",
        "ax.set_xlabel('Episode', fontsize=12)\n",
        "ax.set_ylabel('Success Rate % (rolling avg)', fontsize=12)\n",
        "ax.set_title('Algorithm Comparison: SARSA (Open Goal) vs DDPG (vs Goalie)', fontsize=14)\n",
        "ax.set_ylim(0, 105)\n",
        "ax.legend(loc='lower right', fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add annotation\n",
        "ax.annotate('SARSA: stable, high success\\n(open goal, tile coding)', \n",
        "            xy=(400, sarsa_rolling[-100:].mean()), fontsize=9, alpha=0.7)\n",
        "ax.annotate('DDPG: harder task\\n(goalie blocking, neural net)', \n",
        "            xy=(400, ddpg_rolling.iloc[-100:].mean()), fontsize=9, alpha=0.7)\n",
        "\n",
        "plt.savefig(os.path.join(NOTEBOOKS_PATH, \"fig_sarsa_vs_ddpg.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: fig_sarsa_vs_ddpg.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Cell 19: Final Summary Table & Reproducibility Note ===\n",
        "print(\"=\" * 70)\n",
        "print(\"FINAL EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# SARSA Results\n",
        "print(\"\\nSARSA (Open Goal) - Per-Seed Results:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Seed':<10} | {'Goals':>10} | {'Rate':>10}\")\n",
        "print(\"-\" * 50)\n",
        "for seed in SEEDS:\n",
        "    seed_successes = sum(m['success'] for m in all_metrics[seed])\n",
        "    print(f\"{seed:<10} | {seed_successes:>10} | {100*seed_successes/N_EPISODES:>8.1f}%\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Average':<10} | {'-':>10} | {avg_sarsa:>6.1f}% (±{std_sarsa:.1f})\")\n",
        "print(f\"{'Random':<10} | {'-':>10} | {avg_random:>8.1f}%\")\n",
        "\n",
        "# DDPG Results\n",
        "print(f\"\\nDDPG (vs Goalie):\")\n",
        "print(\"-\" * 50)\n",
        "ddpg_goal_pct = 100*ddpg_successes/DDPG_EPISODES\n",
        "ddpg_blocked_pct = 100*ddpg_blocked/DDPG_EPISODES\n",
        "ddpg_miss = DDPG_EPISODES - ddpg_successes - ddpg_blocked\n",
        "print(f\"  Goals:   {ddpg_successes:>4} / {DDPG_EPISODES} ({ddpg_goal_pct:.1f}%)\")\n",
        "print(f\"  Blocked: {ddpg_blocked:>4} / {DDPG_EPISODES} ({ddpg_blocked_pct:.1f}%)\")\n",
        "print(f\"  Missed:  {ddpg_miss:>4} / {DDPG_EPISODES} ({100*ddpg_miss/DDPG_EPISODES:.1f}%)\")\n",
        "\n",
        "# Output files\n",
        "print(\"\\nOutput files saved to notebooks/\")\n",
        "\n",
        "# Settings\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Experiment Settings\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Seeds: {SEEDS}\")\n",
        "print(f\"  Episodes: {N_EPISODES}\")\n",
        "print(f\"  Noise: ±3° yaw, ±10% speed\")\n",
        "print(f\"  Actions: 28 (7 yaw × 4 speed)\")\n",
        "print(f\"  Goal: 2.4m × 1.8m\")\n",
        "print(f\"  Reward: +5 goal, -5 miss/blocked\")\n",
        "print(\"=\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
